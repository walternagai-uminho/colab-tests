{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMahEZNzUKodm++4d69tbMu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/walternagai-uminho/colab-tests/blob/main/Extract_frames_from_video_and_emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraindo images de vídeos"
      ],
      "metadata": {
        "id": "p0fEAIHfGKcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurações iniciais"
      ],
      "metadata": {
        "id": "IKPmH5zEGXQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bibliotecas necessárias"
      ],
      "metadata": {
        "id": "WvRtmAu8wkte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import imutils\n",
        "from scipy.spatial import distance as dist\n",
        "from imutils import face_utils\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "K2vl1Rq_U4pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Montando a pasta do Google Drive"
      ],
      "metadata": {
        "id": "zLvfJ2mQGt-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbQcz1eeVBa_",
        "outputId": "7a154a06-fe94-4c22-b462-0f6043e827ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WIDTH = 224             # Largura da imagem de saída\n",
        "HEIGHT = 224            # Altura da imagem de saíde\n",
        "EYE_AR_THRESH = 0.18    # Threshold para detectar os olhos das faces\n",
        "\n",
        "AMOSTRAGEM_SEGUNDOS = 10"
      ],
      "metadata": {
        "id": "0aH19LEP562r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abrindo, analisando e retirando imagens do vídeo"
      ],
      "metadata": {
        "id": "U1sSaGwE67k_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando um vídeo e obtendo e suas características"
      ],
      "metadata": {
        "id": "kecU7kibwdk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_file_path = '/content/drive/MyDrive/Colab Notebooks/scrum-sessao-01-beg-01m-02m-30fps.mp4'\n",
        "\n",
        "cap = cv2.VideoCapture(video_file_path)\n",
        "n_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "seconds = frame_count // n_fps\n",
        "minutes = seconds // 60\n",
        "rem_sec = seconds % 60\n",
        "\n",
        "print(f\"n_fps = {n_fps}, frame_count = {frame_count}\")\n",
        "print(f\"seconds = {seconds}, minutes = {minutes}, rem_sec: {rem_sec}\")"
      ],
      "metadata": {
        "id": "V-rP4C_nVB4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3176a5-dc15-4dd0-b78b-9f0846cbcaef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_fps = 30, frame_count = 1802\n",
            "seconds = 60, minutes = 1, rem_sec: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retirando frames do vídeo de acordo com o tempo em segundos"
      ],
      "metadata": {
        "id": "RegMA_L16eFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "![ ! -d \"extracted_images\" ] && mkdir -p \"extracted_images\"\n",
        "\n",
        "image_list = []\n",
        "success, image = cap.read()\n",
        "frame_count = 0\n",
        "\n",
        "while success:\n",
        "    if (frame_count % (n_fps * AMOSTRAGEM_SEGUNDOS)) == 0:\n",
        "        frame_time = frame_count // n_fps\n",
        "        filename = \"/content/extracted_images/image_{}.jpg\".format(int(frame_time))\n",
        "        cv2.imwrite(filename, image)\n",
        "        image_list.append(filename)\n",
        "\n",
        "    frame_count = frame_count + 1\n",
        "    success, image = cap.read()\n",
        "  \n",
        "cap.release()"
      ],
      "metadata": {
        "id": "axBjcnLRgi9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detectando faces usando haarcascade"
      ],
      "metadata": {
        "id": "vWL4Kvpt6z0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtendo arquivos para detecção de rostos e olhos"
      ],
      "metadata": {
        "id": "UuvUuC-o6GLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cascade  \n",
        "![ !\"haarcascade_frontalface_default.xml\" ] && wget \"https://github.com/kipr/opencv/raw/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "![ !\"haarcascade_eye.xml\" ] && wget \"https://github.com/kipr/opencv/raw/master/data/haarcascades/haarcascade_eye.xml\"\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')  \n",
        "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')"
      ],
      "metadata": {
        "id": "BWNExH1PnP6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c27e068e-3ccf-404d-c1d6-49cc1ee6047f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-15 14:41:20--  https://github.com/kipr/opencv/raw/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/kipr/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml [following]\n",
            "--2023-04-15 14:41:20--  https://raw.githubusercontent.com/kipr/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1254733 (1.2M) [text/plain]\n",
            "Saving to: ‘haarcascade_frontalface_default.xml’\n",
            "\n",
            "haarcascade_frontal 100%[===================>]   1.20M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-04-15 14:41:21 (33.2 MB/s) - ‘haarcascade_frontalface_default.xml’ saved [1254733/1254733]\n",
            "\n",
            "--2023-04-15 14:41:21--  https://github.com/kipr/opencv/raw/master/data/haarcascades/haarcascade_eye.xml\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/kipr/opencv/master/data/haarcascades/haarcascade_eye.xml [following]\n",
            "--2023-04-15 14:41:21--  https://raw.githubusercontent.com/kipr/opencv/master/data/haarcascades/haarcascade_eye.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 506314 (494K) [text/plain]\n",
            "Saving to: ‘haarcascade_eye.xml’\n",
            "\n",
            "haarcascade_eye.xml 100%[===================>] 494.45K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-04-15 14:41:21 (20.5 MB/s) - ‘haarcascade_eye.xml’ saved [506314/506314]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtendo as faces e olhos das imagens"
      ],
      "metadata": {
        "id": "ey6bI_EK6P03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "![ ! -d \"extracted_faces\" ] && mkdir -p \"extracted_faces\"\n",
        "\n",
        "image_count = 1\n",
        "face_list = []\n",
        "for image in image_list:\n",
        "    img = cv2.imread(image)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(img, 1.1, 6)  \n",
        "\n",
        "    face_count = 1\n",
        "    for (x, y, w, h) in faces:  \n",
        "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)  \n",
        "        roi_gray = gray[y:y + h, x:x + w]\n",
        "        roi_gray_resized = cv2.resize(roi_gray, (WIDTH, HEIGHT))\n",
        "\n",
        "        roi_color = img[y:y + h, x:x + w]\n",
        "        roi_color_resized = cv2.resize(roi_color, (WIDTH, HEIGHT))\n",
        "        eyes = eye_cascade.detectMultiScale(roi_color_resized)\n",
        "        if len(eyes) > 1:\n",
        "            for (ex,ey,ew,eh) in eyes:\n",
        "                print(f\"{ex} {ey} {ew} {eh}\")\n",
        "                if ew > 35 and ew < 82:\n",
        "                    cv2.rectangle(roi_color_resized,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
        "                    cv2_imshow(roi_color_resized)\n",
        "\n",
        "            filename = \"/content/extracted_faces/\" + image.split('/')[-1] + f\"-{image_count}\" + \"-\" + str(face_count) + \".jpg\"\n",
        "            cv2.imwrite(filename, roi_gray_resized)\n",
        "            face_list.append(filename)\n",
        "\n",
        "        face_count = face_count + 1\n",
        "    image_count = image_count + 1\n",
        "    \n",
        "    #cv2_imshow(img)"
      ],
      "metadata": {
        "id": "vER2oWbEs9bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detectando faces usando a biblioteca dlib"
      ],
      "metadata": {
        "id": "2cTbNAqxXEIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criando pastas"
      ],
      "metadata": {
        "id": "SlNsUihkG0h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "![ ! -d \"faces_attention\" ] && mkdir -p \"faces_attention\"\n",
        "![ ! -d \"faces_noattention\" ] && mkdir -p \"faces_noattention\""
      ],
      "metadata": {
        "id": "ka6bxnuZ5mWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando arquivo de dados para detecção de faces"
      ],
      "metadata": {
        "id": "yyEAqjp0G96k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/italojs/facial-landmarks-recognition\n",
        "# https://github.com/JeffTrain/selfie\n",
        "\n",
        "![ !\"shape_predictor_68_face_landmarks.dat\" ] && wget https://github.com/JeffTrain/selfie/raw/master/shape_predictor_68_face_landmarks.dat\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "detector_pontos = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "distancia_olhos_atencao = (20, 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI8GjDdMW6fT",
        "outputId": "fa2a0565-947b-4ad3-c7e5-99487e8349ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-14 19:28:29--  https://github.com/JeffTrain/selfie/raw/master/shape_predictor_68_face_landmarks.dat\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/JeffTrain/selfie/master/shape_predictor_68_face_landmarks.dat [following]\n",
            "--2023-04-14 19:28:30--  https://raw.githubusercontent.com/JeffTrain/selfie/master/shape_predictor_68_face_landmarks.dat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99693937 (95M) [application/octet-stream]\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  95.08M   233MB/s    in 0.4s    \n",
            "\n",
            "2023-04-14 19:28:31 (233 MB/s) - ‘shape_predictor_68_face_landmarks.dat’ saved [99693937/99693937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detecção dos olhos das faces\n",
        "\n",
        "Cada olho é representado por 6 coordenadas (x, y), começando no canto esquerdo do olho (como se você estivesse olhando para a pessoa) e, em seguida, trabalhando no sentido horário ao redor do restante da região:\n",
        "\n",
        "[Imagem das 6 coordenadas (x, y)](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2017/04/blink_detection_6_landmarks.jpg?lossy=1&strip=1&webp=1)\n",
        "\n",
        "Esse cálculo considera os pontos de referência (landmarks) descrito no trabalho de [Soukupová and Čech in their 2016 paper](http://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf)\n",
        "\n",
        "$ear = \\frac{||p_2 - p_6|| + ||p_3 - p_5||}{2 * ||p_1 - p_4||}$\n",
        "\n",
        "$ear$ é a razão de aspecto de olho ou *eye aspect ratio*."
      ],
      "metadata": {
        "id": "QFVAajbVHKuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/\n",
        "\n",
        "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
        "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "\t# compute the euclidean distances between the two sets of\n",
        "\t# vertical eye landmarks (x, y)-coordinates\n",
        "\tA = dist.euclidean(eye[1], eye[5])\n",
        "\tB = dist.euclidean(eye[2], eye[4])\n",
        "\t# compute the euclidean distance between the horizontal\n",
        "\t# eye landmark (x, y)-coordinates\n",
        "\tC = dist.euclidean(eye[0], eye[3])\n",
        "\t# compute the eye aspect ratio\n",
        "\tear = (A + B) / (2.0 * C)\n",
        "\t# return the eye aspect ratio\n",
        "\treturn ear"
      ],
      "metadata": {
        "id": "CB49Tv7d49Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_list = []\n",
        "image_count = 1\n",
        "for image in image_list:\n",
        "    img = cv2.imread(image)\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(img_gray, 1)\n",
        "\n",
        "    face_count = 1\n",
        "    for face in faces:  \n",
        "        # determine the facial landmarks for the face region, then\n",
        "\t\t# convert the facial landmark (x, y)-coordinates to a NumPy\n",
        "\t\t# array\n",
        "        pontos = detector_pontos (img_gray, face)\n",
        "        pontos = face_utils.shape_to_np(pontos)\n",
        "\n",
        "\t\t# extract the left and right eye coordinates, then use the\n",
        "\t\t# coordinates to compute the eye aspect ratio for both eyes\n",
        "        leftEye = pontos[lStart:lEnd]\n",
        "        rightEye = pontos[rStart:rEnd]\n",
        "        leftEAR = eye_aspect_ratio(leftEye)\n",
        "        rightEAR = eye_aspect_ratio(rightEye)\n",
        "\n",
        "\t\t# average the eye aspect ratio together for both eyes\n",
        "        ear = (leftEAR + rightEAR) / 2.0\n",
        "\n",
        "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
        "        face_gray = img_gray[y:y + h, x:x + w]\n",
        "        face_gray_resized = cv2.resize(face_gray, (WIDTH, HEIGHT))\n",
        "\n",
        "        # if ear greater or equal EYV_AR_THRESHOLD the eyes are open\n",
        "        if ear >= EYE_AR_THRESH:\n",
        "            filename = \"faces_attention/\" + image.split('/')[-1] + \"\".format(image_count) + \"-\" + str(face_count) + \".jpg\"\n",
        "            \n",
        "            face_list.append(filename)\n",
        "            cv2.imwrite(filename, face_gray_resized)\n",
        "\n",
        "            # compute the convex hull for the left and right eye, then\n",
        "            # visualize each of the eyes\n",
        "            leftEyeHull = cv2.convexHull(leftEye)\n",
        "            rightEyeHull = cv2.convexHull(rightEye)\n",
        "\n",
        "            cv2.drawContours(img, [leftEyeHull], -1, (0, 255, 0), 1)\n",
        "            cv2.drawContours(img, [rightEyeHull], -1, (0, 255, 0), 1)\n",
        "\n",
        "            CLASS = \"attention\"\n",
        "        # Eyes are closed or blink\n",
        "        else:\n",
        "            filename = \"faces_noattention/\" + image.split('/')[-1] + f\"-{image_count}\" + \"-\" + str(face_count) + \".jpg\"\n",
        "            cv2.imwrite(filename, face_gray_resized)\n",
        "            CLASS = \"\\tnoattention\"\n",
        "        \n",
        "        #print(f\"ear: {ear} - image_count: {image_count} - face_count: {face_count} - CLASS: {CLASS}\")\n",
        "\n",
        "        face_count = face_count + 1\n",
        "    image_count = image_count + 1\n",
        "\n",
        "    cv2_imshow(img)\n"
      ],
      "metadata": {
        "id": "DnoyPYWGZt_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detectando faces usando o DeepFace"
      ],
      "metadata": {
        "id": "J7LZCV1dOdZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalando a biblioteca DeepFace"
      ],
      "metadata": {
        "id": "DAi-K3XLOhiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3is5VxbOpso",
        "outputId": "374c1e8b-2667-48c1-b3c1-c4a416b4a935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepface\n",
            "  Downloading deepface-0.0.79-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from deepface) (2.2.3)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from deepface) (2.12.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.9/dist-packages (from deepface) (4.7.0.72)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from deepface) (2.12.0)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.9/dist-packages (from deepface) (1.5.3)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.9/dist-packages (from deepface) (4.6.6)\n",
            "Collecting fire>=0.4.0\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.9/dist-packages (from deepface) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from deepface) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.9/dist-packages (from deepface) (4.65.0)\n",
            "Collecting mtcnn>=0.1.0\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting retina-face>=0.0.1\n",
            "  Downloading retina_face-0.0.13-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from fire>=0.4.0->deepface) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire>=0.4.0->deepface) (2.2.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=1.1.2->deepface) (2.1.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=1.1.2->deepface) (3.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=1.1.2->deepface) (8.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=1.1.2->deepface) (6.3.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/dist-packages (from Flask>=1.1.2->deepface) (2.2.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=3.10.1->deepface) (4.11.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown>=3.10.1->deepface) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=3.10.1->deepface) (3.11.0)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.9/dist-packages (from gunicorn>=20.1.0->deepface) (67.6.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.4->deepface) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (23.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (0.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (2.12.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (23.3.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (2.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (1.14.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (1.53.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (4.5.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (0.4.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (16.0.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.9.0->deepface) (3.20.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.40.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->Flask>=1.1.2->deepface) (3.15.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=1.9.0->deepface) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=1.9.0->deepface) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->Flask>=1.1.2->deepface) (2.1.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (0.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (2.17.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.26.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (3.2.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=90845c6d764015acaf5e0cfef86162ea5efa7ac12b218ddb8afcf594a35e3a77\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "Successfully built fire\n",
            "Installing collected packages: gunicorn, fire, mtcnn, retina-face, deepface\n",
            "Successfully installed deepface-0.0.79 fire-0.5.0 gunicorn-20.1.0 mtcnn-0.1.1 retina-face-0.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepface import DeepFace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTygqQiwOvmu",
        "outputId": "3ad4bc97-2645-4699-dda4-e15dc12dbbae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory  /root /.deepface created\n",
            "Directory  /root /.deepface/weights created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métricas de comparação"
      ],
      "metadata": {
        "id": "TCpo9EtEZNK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [\"cosine\", \"euclidean\", \"euclidean_l2\"]"
      ],
      "metadata": {
        "id": "Zv-zgeLeZQ5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detectando faces usando models"
      ],
      "metadata": {
        "id": "TFSlI5ksZAve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "  \"VGG-Face\", \n",
        "  \"Facenet\", \n",
        "  \"Facenet512\", \n",
        "  \"OpenFace\", \n",
        "  \"DeepFace\", \n",
        "  \"DeepID\", \n",
        "  \"ArcFace\", \n",
        "  \"Dlib\", \n",
        "  \"SFace\",\n",
        "]"
      ],
      "metadata": {
        "id": "mvFo1th3ZGmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detectando faces usando backends\n",
        "\n",
        "A detecção e alinhamento facial são estágios iniciais importantes de um pipeline de reconhecimento facial moderno. Experimentos mostram que apenas o alinhamento aumenta a precisão do reconhecimento facial em quase 1%. Os detectores OpenCV, SSD, Dlib, MTCNN, RetinaFace e MediaPipe são envolvidos em deepface."
      ],
      "metadata": {
        "id": "cbVQdIYdPOif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backends = [\n",
        "  'opencv', \n",
        "  'ssd', \n",
        "  'dlib', \n",
        "  'mtcnn', \n",
        "  'retinaface', \n",
        "  'mediapipe'\n",
        "]"
      ],
      "metadata": {
        "id": "lelktfaCO91R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#face verification\n",
        "for i in range(len(face_list)-1):\n",
        "    obj = DeepFace.verify(img1_path = face_list[i], \n",
        "                        img2_path = face_list[i+1], \n",
        "                        enforce_detection = False,\n",
        "                        detector_backend = backends[0])\n",
        "    if obj[\"verified\"] == False:\n",
        "        print(f\"{face_list[i]} is different {face_list[i+1]}\")"
      ],
      "metadata": {
        "id": "Q-WVZyG8Vv0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detectando emoções a partir das imagens"
      ],
      "metadata": {
        "id": "xhLb99SKQFSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando o FER\n",
        "\n",
        "https://github.com/atulapra/Emotion-detection\n",
        "\n",
        "https://www.edlitera.com/blog/posts/emotion-detection-in-images\n",
        "\n",
        "https://towardsdatascience.com/the-ultimate-guide-to-emotion-recognition-from-facial-expressions-using-python-64e58d4324ff\n",
        "\n",
        "https://github.com/rjrahul24/ai-with-python-series\n",
        "\n",
        "https://github.com/rjrahul24/ai-with-python-series/tree/main/06.%20Emotion%20Recognition%20using%20Facial%20Images\n",
        "\n",
        "https://github.com/rjrahul24/ai-with-python-series/tree/main/07.%20Emotion%20Recognition%20using%20Live%20Video"
      ],
      "metadata": {
        "id": "y1zcUwwgQL7o"
      }
    }
  ]
}